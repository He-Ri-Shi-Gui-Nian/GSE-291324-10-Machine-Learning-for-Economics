# Decomposition of the conditional MSE
$$Y = m(X) + \varepsilon,\qquad \mathrm{E}[\varepsilon \mid X=x] = 0,\qquad \mathrm{Var}(\varepsilon \mid X=x) = \sigma^2(x)$$

Let
$$\mu(x) := \mathrm{E}[\hat{f}(x) \mid X=x]$$

### Step 1: Add and subtract $\mu(x)$
$$
\begin{aligned}
\mathrm{E}[(Y - \hat{f}(x))^2 \mid X=x]
&= \mathrm{E}[(Y - \mu(x) + \mu(x) - \hat{f}(x))^2 \mid X=x] \\
&= \mathrm{E}[(Y - \mu(x))^2 \mid X=x] + \mathrm{E}[(\mu(x) - \hat{f}(x))^2 \mid X=x] \\
&\quad + 2\,\mathrm{E}[(Y - \mu(x))(\mu(x) - \hat{f}(x)) \mid X=x]
\end{aligned}
$$

### Step 2: Cross term vanishes (This can also be done by first expanding using $Y = m(x) + \varepsilon$ and then plugging in the definition $\mu(x) := \mathrm{E}[\hat{f}(x) \mid X=x]$)
$$
\begin{aligned}
\mathrm{E}[(Y - \mu(x))(\mu(x) - \hat{f}(x)) \mid X=x]
&= \mathrm{E}[Y - \mu(x) \mid X=x]\ \mathrm{E}[\mu(x) - \hat{f}(x) \mid X=x] \\
&= 0
\end{aligned}
$$

Thus

$$
\begin{aligned}
\mathrm{E}[(Y - \hat{f}(x))^2 \mid X=x]
&= \mathrm{E}[(Y - \mu(x))^2 \mid X=x] + \mathrm{Var}(\hat{f}(x) \mid X=x)
\end{aligned}
$$

### Step 3: Expand using $Y = m(x) + \varepsilon$
$$
\begin{aligned}
\mathrm{E}[(Y - \mu(x))^2 \mid X=x]
&= \mathrm{E}[(m(x) + \varepsilon - \mu(x))^2 \mid X=x] \\
&= (m(x) - \mu(x))^2 + \mathrm{Var}(\varepsilon \mid X=x)
\end{aligned}
$$

### Final Result (Biasâ€“Variance Decomposition)
$$\boxed{\mathrm{E}[(Y - \hat{f}(x))^2 \mid X=x] = \mathrm{Var}(\hat{f}(x)\mid X=x) + \big(m(x) - \mathrm{E}[\hat{f}(x)\mid X=x]\big)^2 + \mathrm{Var}(\varepsilon\mid X=x)}$$


## Footnote
$$ Var(\hat f(x)) = E\left[ \left(\hat f(x) - E[\hat f(x)] \right)^2 \right]$$

# Decision Tree
The construction of regression trees generally consists of two main steps.
- Step 1 Divide the feature space $X$ of $(X_1, X_2, . . . , X_p )$ into J distinct and non-overlapping regions, $R_1, R_2, . . . , R_J$.
- Step 2 Compute the mean of the outcomes $Y_i$ for the observations belonging to $R_j$. This value becomes the prediction of the outcomes for any features $x$ belonging to $R_j$.

## Greedy algorithm
Possible to have two or more internal nodes with the same feature

## P.S.
Keep in mind that everything on the slides is the CART algorithm. 
